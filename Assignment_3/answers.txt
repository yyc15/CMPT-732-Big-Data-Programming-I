1.	What was wrong with the original wordcount-5 data set that made repartitioning worth it? Why did the program run faster after?Ans:As the data sets of wordcount-5 are not evenly distributed, by using repartitioning, the data set can be splitted into multiple partitions evenly. Therefore, the transformation can be executed on multiple partitions in parallel. As a result, the program can run faster. However, it also depends on the number of partitions decided in the program as appropriate number of partitions is required for optimum performance. If the number is too small which means each partition is huge and not all the cluster resources are fully untilised, the efficiency will be low. On the other hand, if the number is too large , Spark has to create relative number of tasks. Thus, the run time may be even longer as creating, scheduling, and managing the tasks before executing takes time too.2.	The same fix does not make this code run faster on the wordcount-3 data set. (It may be slightly slower?) Why? [For once, the answer is not “the data set is too small”.]Ans:In repartitioning , the data has to be partitioned and shuffling. Thus, repartitioning would make the process longer since it adds overhead to the process. Also, the data sets in wordcount-3 are comparatively evenly distributed. Therefore, the process may run even faster if there is no repartitioning if the file is small.3.	How could you modify the wordcount-5 input so that the word count code can process it and get the same results as fast as possible? (It's possible to get about another minute off the running time.)Ans:Since the data sets of wordcount-5 are large and not evenly distributed, we can repartition the input file first so that the input data sets are evenly distributed. 4.	When experimenting with the number of partitions while estimating Euler's constant, you likely didn't see much difference for a range of values, and chose the final value in your code somewhere in that range. What range of partitions numbers was “good” (on the desktop/laptop where you were testing)? Ans:I have tested locally on my laptop for n=100000000.Here are the result respectively:numSlices=1 : 1m3.822s  numSlices=5: 43.540s  numSlices=6: 42.898s  numSlices=10: 39.993s  numSlices=30: 39.672snumSlices=100: 43.640Therefore, the range between numSlices=10 to numSlices=30 was good.5.	How much overhead does it seem like Spark adds to a job? How much speedup did PyPy get over the usual Python implementation?Ans:For n = 1000000000,Result of standard CPython with Spark, the duration is 5m55.959sResult of PyPy with Spark, the duration is 0m59.446sResult of non-spark single-threaded PyPy, the duration is 0m45.526sResult of non-spark single-threaded C , the duration is 0m25.907sTherefore, Spark has added quite a number of overhead to a job since without using Spark the run time is significantly faster. Also, by comparing CPython with Spark and PyPy with Spark, PyPy with Spark has significantly shorter duration. By this example, it is around 5 -6 times faster.